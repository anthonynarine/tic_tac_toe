# File: langchain_agent.py

import os
import logging
from pathlib import Path
from django.conf import settings

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import OpenAI

logger = logging.getLogger(__name__)

# === üìÅ Project Base Directory (Compatible with Heroku & Local Dev) ===
if settings.DEBUG:
    PROJECT_ROOT = Path(__file__).parents[2].resolve()  # ‚úÖ Dev: /yourproject/backend
else:
    PROJECT_ROOT = Path(__file__).resolve().parent.parent  # ‚úÖ Heroku: /app/backend

logger.info(f"[DEBUG] PROJECT_ROOT resolved to: {PROJECT_ROOT}")

if settings.DEBUG:
    logger.info("üîß DEBUG mode ON ‚Äî including frontend directories for dev.")
    TARGET_DIRS = [
        PROJECT_ROOT / "backend" / "ai_agent",
        PROJECT_ROOT / "backend" / "chat",
        PROJECT_ROOT / "backend" / "friends",
        PROJECT_ROOT / "backend" / "game",
        PROJECT_ROOT / "backend" / "users",
        PROJECT_ROOT / "backend" / "utils",
        PROJECT_ROOT / "tic-tac-toe" / "src" / "components",
        PROJECT_ROOT / "tic-tac-toe" / "src" / "reducers",
        PROJECT_ROOT / "tic-tac-toe" / "src" / "context",
    ]
else:
    logger.info("üöÄ Production mode ‚Äî loading all backend apps for LangChain agent.")
    TARGET_DIRS = [
        PROJECT_ROOT / "ai_agent",
        PROJECT_ROOT / "chat",
        PROJECT_ROOT / "friends",
        PROJECT_ROOT / "game",
        PROJECT_ROOT / "users",
        PROJECT_ROOT / "utils",
    ]

# Debug log to validate path resolution
for path in TARGET_DIRS:
    logger.debug(f"[CHECK] Path: {path} | Exists: {path.exists()}")

# Optional: Print path check for dev
# for dir in TARGET_DIRS:
#     print(f"[DEBUG] Checking dir exists: {dir} ‚Üí {dir.exists()}")

# === üö´ Filters ===
EXCLUDE_DIRS = {
    "migrations", "__pycache__", "static", "media", "node_modules",
    "public", "build", ".git", ".vscode",
}
EXCLUDE_FILES = {
    "settings.py", "secrets.py", ".env", "manage.py", "apps.py",
    "admin.py", "tokens.py", "permissions.py", "serializers.py",
}
VALID_EXTENSIONS = {".py", ".md", ".txt", ".jsx", ".tsx", ".js", ".ts"}

# === üìÑ Load Project Files ===
def load_project_documents():
    logger.info("üìö Starting document loading process.")
    documents = []

    for base_dir in TARGET_DIRS:
        base_dir_str = str(base_dir)
        logger.info(f"üîç Scanning directory: {base_dir_str}")
        if not os.path.exists(base_dir_str):
            logger.warning(f"‚ö†Ô∏è Directory does not exist: {base_dir_str}")
            continue

        for root, dirs, files in os.walk(base_dir_str):
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
            for file in files:
                file_path = os.path.join(root, file)
                ext = os.path.splitext(file)[-1]

                if file in EXCLUDE_FILES:
                    logger.debug(f"‚è≠Ô∏è Skipping excluded file: {file_path}")
                    continue
                if ext not in VALID_EXTENSIONS:
                    logger.debug(f"üö´ Unsupported extension {ext}: {file_path}")
                    continue

                try:
                    loader = TextLoader(file_path, encoding="utf-8")
                    loaded_docs = loader.load_and_split()
                    for doc in loaded_docs:
                        doc.metadata["source"] = file_path  # ‚úÖ Add metadata for traceability
                    documents.extend(loaded_docs)
                    logger.debug(f"‚úÖ Loaded {len(loaded_docs)} docs from: {file_path}")
                except Exception as e:
                    logger.error(f"‚ùå Failed to load {file_path}: {e}")

    logger.info(f"üèÅ Document loading complete. Total loaded: {len(documents)}")
    return documents

# === ü§ñ Build the LangChain Agent ===
def build_agent():
    logger.info("üß† Building LangChain agent.")
    docs = load_project_documents()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  # ‚úÖ Larger chunks
    chunks = splitter.split_documents(docs)
    logger.debug(f"Chunks created: {len(chunks)}")

    if not chunks:
        logger.error("üõë No document chunks available. Aborting agent build.")
        raise ValueError("No document chunks to embed. Check document loading.")

    # üîê Ensure API key is present
    if not settings.OPENAI_API_KEY:
        logger.critical("‚ùå OPENAI_API_KEY is missing from settings.")
        raise ValueError("OPENAI_API_KEY is not set. Add it to your .env file.")

    embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)
    logger.info("üîë OpenAI Embeddings initialized.")

    vectorstore = FAISS.from_documents(chunks, embeddings)
    logger.info("üì¶ Vectorstore created using FAISS.")

    retriever = vectorstore.as_retriever()
    logger.info("üì° Retriever ready.")

    chain = RetrievalQA.from_chain_type(  # üîÅ Switch to RetrievalQAWithSourcesChain if desired
        llm=OpenAI(
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        ),
        chain_type="stuff",
        retriever=retriever,
    )

    logger.info("‚úÖ LangChain RetrievalQA chain successfully built.")
    return chain
